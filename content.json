[{"title":"数字识别","date":"2019-08-11T13:34:32.369Z","path":"2019/08/11/数字识别/","text":"mnist数据集：包含了七万多张黑底白字手写数字照片，其中55000张为训练集，5000张为验证集，10000张为测试集。每张大小为28*28像素，图片中纯黑色像素为0，纯白色为1，数据集的长度是10的一维数组，数组中的每个元素索引号表示对应数字出现的概率。在将mnist数据集作为输入喂入神经网络是，需要将数据集中的每一张照片变为长度为784的一维数组。例如：一张数字手写字体图片的长度是784的一维数组[0,0,0,0.2,0.5,…,0]输入神经网络。该图片对应的标签为[0,0,0,0,0,1,0,0,0,0]，表示数字6出现的概率是100%，那么图片对应的识别结果是6 加载数据集：使用input_data模块中的read_data_sets()函数加载 from tensorflow.examples/tutorials.mnist import input_datamnist = input_data.read_data_sets(‘./data/‘,one_hot = true)，其中第一个参数表示数据集存放的路径，第二个参数表示数据集的存取形式，当为True时，表示以独热码形式存取数据集 独热码：直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制 read_data_sets()函数运行时，会检查指定路径内是否已经有数据集，若没有数据集就会自动下载，并将mnist数据集分为训练、验证、测试集。 数据集的操作(以训练集为例)：返回样本数： print “train data size:”,mnist.train.mun_examples 返回标签集：mnist.train.labels[0],返回第0张图的标签 返回像素值：mnist.train.images[0]，返回第0张图的像素值 将数据输入神经网络：xs,ys=mnist.train.next_batch(BATCH_SIZE),其中BATCH_SIZE表示随机从训练集中抽取BATCH_SIZE个样本输入神经网路，并将样本的像素值和标签分别赋值给xs，ys。在本实验中，BATCH_SIZE设置为200，表示一次将200个样本的像素值和标签分别赋值给xs和ys，xs的形状是(200,784),ys的形状是（200，10） 实现手写识别的常用函数 tf.get_collection(“”)：从collection集合中取出全部变量生成一个列表 tf.add()：将参数列表中对应元素相加 tf.cast(x,dtype)：将参数x转换为指定数据类型dtype tf.equal()：对比两个矩阵或者向量的元素。若对应元素相同则为True。 tf.random_mean(x,axis)：表示求取矩阵或者张量指定维度的平均值。若不指定第二个参数，则在所有元素中取平均值；若指定为0，则在第一维元素上取平均值，即每一列取平均值；若指定第二个参数为1，则在第二维元素上取平均值，即每一行求平均值。 tf.argmax(x,axis)：返回指定维度axis下，参数x中最大值索引号 os.path.join()：表示把参数字符串按照路径命名规则拼接 tf.Graph().as_default() 函数表示当前图设置为默认图，并返回一个上下文管理器。该函数一般与with关键字搭配使用，应用于将已经定义好的神经网路在计算图中实现 神经网路模型的保存： 在反向传播中，一般会间隔一定轮数保存一次神经网络模型，并产生三个文件:保存当前图结构的.meta文件、保存当前参数名的.index文件、保存当前参数的.data文件 12345saver= tf.train.Saver()with tf.Session() as sess: for i in range(STEPs): if i % lunNum ==0: saver.save(sess,os.path.join(....),global_step=global_step) 其中，tf.train.Saver()用来实例化一个saver对象，在经过指定轮数后，将所有的参数保存到指定路径上，并且在存放网络模型的文件夹名称中注明保存模型时的训练轮数 神经网络模型的加载： 在测试网络效果时，需要将训练好的神经网络模型加载，在tensorflow中： 1234with tf.Session() as sess: ckpt = tf.train.get_checkpoint_state(存储路径) if ckpt and ckpt.model_checkpoint_path: saver.restore(sess,ckpt.model_checkpoint_path) 在with结构中进行加载保存的网络模型，若有，就将其加载到当前的会话中 加载模型中的滑动平均值 在保存模型时，若模型中采用滑动平均，则参数的滑动平均值会保存在相应的文件中，通过实例化saver对象，实现参数滑动平均值的加载，在tensorflow中表示如下： 123ema = tf.train.ExponentialMovingAverage(滑动平均基数)ema_score = ema.variable_to_restore()saver = tf.train.Saver(ema_restore) 神经网络模型的准确率评估方法： 12correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(corrent_prediction,tf.float32)) y表示在一组数据中的预测结果，形状为[batch_size,10],每一行表示一张图片的识别结果，通过softmax取出每张图片对应向量中最大值对应的索引值，组成[batch,1]。然后通过equal()来判断是否相等，用cast()把True和False转变成1和0，然后用reduce_mean()求平均值，即可得到准确率 前向传播需要定义神经网络中的参数w和偏执b，定义由输入到输出的网络结构 123456789101112131415161718192021222324import tensorflow as tfINPUT_NODE= 784OUTPUT_NODE= 10LAYER1_NODE= 500def get_weight(shape,regularizer): w = tf.Variable(tf.truncated_normal(shape,stddev=0.1)) if regularizer != None: tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w)) #正则化 return wdef get_bias(shape): b = tf.Variable(tf.zeros(shape)) return bdef forward(x,regularizer): w1 = get_weight([INPUT_NODE,LAYER1_NODE],regularizer) b1 = get_bias([LAYER1_NODE]) y1 = tf.nn.relu(tf.matmul(x,w1)+b1) w2 = get_weight([LAYER1_NODE,OUTPUT_NODE],regularizer) b2 = get_bias([OUTPUT_NODE]) y = tf.matmul(y1,w2)+b2 return y 反向传播在反向传播过程中，用tf.placeholder(dtype,shape)来是实现对训练样本x和样本标签y_的占位。y表示在前向传播里面定义的forward()；损失函数loss，一般为预测值与样本值的交叉熵（均方误差）与正则化损失之和。train_step表示利用优化算法对参数模型进行优化，常用的优化算法GradientDescentOptimizer、AdamOptimizer、MomentumOptimizer算法。然后实例化Saver，利用tf.initialize_all_variables().run()来实例化所有参数，利用sess.run()实现模型的训练优化过程，并且每隔一定轮数保存一次模型 正则化、指数衰减学习、滑动平均方法的设置 正则化regularization:首先，需要在前向传播过程中加入 12if regularizer != None: tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w)) #正则化 然后，需要在反向传播过程中加入 123ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels = tf.argmax(y_,1)) cem = tf.reduce_mean(ce) loss = cem+tf.add_n(tf.get_collection(&apos;losses&apos;)) 指数衰减学习率 保证了前期快速收敛，后期不会有太大波动 1learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,mnist.train.num_examples/BATCH_SIZE,LEARNING_RATE_DECAY,staircase = True) 滑动平均 使得具有更好的泛化能力 1234ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step) ema_op = ema.apply(tf.trainable_variables()) with tf.control_dependencies([train_step,ema_op]): train_op=tf.no_op(name=&quot;train&quot;) 下面是全部代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport mnist_forwardimport osBATCH_SIZE = 200LEARNING_RATE_BASE = 0.1LEARNING_RATE_DECAY = 0.99REGULARIZER = 0.0001STEPS = 50000MOVING_AVERAGE_DECAY = 0.99MODEL_SAVE_PATH = &quot;./model/&quot;MODEL_NAME = &quot;mnist_model&quot;def backward(mnist): x= tf.placeholder(tf.float32,[None,mnist_forward.INPUT_NODE]) y_ = tf.placeholder(tf.float32,[None,mnist_forward.OUTPUT_NODE]) y = mnist_forward.forward(x,REGULARIZER) global_step = tf.Variable(0,trainable = False) ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels = tf.argmax(y_,1)) cem = tf.reduce_mean(ce) loss = cem+tf.add_n(tf.get_collection(&apos;losses&apos;)) learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,mnist.train.num_examples/BATCH_SIZE,LEARNING_RATE_DECAY,staircase = True) train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step) ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step) ema_op = ema.apply(tf.trainable_variables()) with tf.control_dependencies([train_step,ema_op]): train_op=tf.no_op(name=&quot;train&quot;) saver=tf.train.Saver() with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) for i in range(STEPS): xs,ys = mnist.train.next_batch(BATCH_SIZE) _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict=&#123;x:xs,y_:ys&#125;) if i%1000==0: print(&quot;After %d training strp(s),loss on training batch is %g.&quot;%(step,loss_value)) saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)def main(): mnist = input_data.read_data_sets(&quot;./data/&quot;,one_hot = True) print (&quot;gggg&quot;) backward(mnist)if __name__ == &apos;__main__&apos;: main() 测试12345678910111213141516171819202122232425262728293031323334353637import timeimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport mnist_forwardimport mnist_backwardTEST_INTERVAL_SECS = 5def test(mnist): with tf.Graph().as_default() as g: x= tf.placeholder(tf.float32,[None,mnist_forward.INPUT_NODE]) #占位 y_ = tf.placeholder(tf.float32,[None,mnist_forward.OUTPUT_NODE]) y = mnist_forward.forward(x,None) #前向传播得到的y ema = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY) ema_restore = ema.variables_to_restore() saver = tf.train.Saver(ema_restore) correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) while True: with tf.Session() as sess: #加载训练好的模型 ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH) if ckpt and ckpt.model_checkpoint_path: saver.restore(sess,ckpt.model_checkpoint_path)#恢复会话 global_step = ckpt.model_checkpoint_path.split(&apos;/&apos;)[-1].split(&apos;-&apos;)[-1] #恢复轮数 accuracy_score = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y_:mnist.test.labels&#125;) #计算准确率 print(&quot;After %s training step(s),test accuracy = %g&quot;%(global_step,accuracy_score)) else: print(&quot;NO checkpoint file found&quot;) return time.sleep(TEST_INTERVAL_SECS)def main(): mnist = mnist = input_data.read_data_sets(&quot;./data/&quot;,one_hot = True) test(mnist)if __name__ == &apos;__main__&apos;: main()","comments":true,"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/神经网络/"}]},{"title":"神经网络优化","date":"2019-08-09T14:57:49.868Z","path":"2019/08/09/搭建神经网络优化/","text":"损失函数 神经元模型：用数学公式表示为：$f(\\sum_ix_iw_i+b)$，f为激活函数。神经网络是以神经元为基本单元构成的。 激活函数：引入非线性激活因素，以提高模型的表达力。常用的激活函数有relu,sigmoid,tanh.等 relu：在tensorflow中使用tf.nn.relu()表示 sigmoid：在tensorflow中使用tf.nn.sigmoid tanh:在tensorflow中使用tf.nn.sigmoid 神经网路的复杂度：可用神经网络的层数和神经网络中待优化参数的个数表示 神经网络的层数：一般不计入输入层，层数 = n个隐藏层+1个输出层 神经网络待优化的参数：神经网络中所有参数w的个数+所有参数b的个数 损失函数：用来表示预测值y和已知答案y_的差距。在训练神经网络时，通过不断的改变神经网络中所有参数，在使损失函数不断减小，从而训练出更高准确率的神经网络模型 常用的损失函数：均方误差、自定义、交叉熵 均方误差mse：n个样本的预测值y与已知样本y_之差的平方和，再求平均值$MSE(y_,y)\\frac{\\sum _{i=1}^n(y-y_)^2}{n}$在tensorflow中使用loss_mse = tf.reduce_mean(tf.square(y__-y)) 。 自定义损失函数：使用方式是loss= tf.reduce_sum(tf.where(tf.greater(y,y_),f1(y-y_),f2(y-y_))) 交叉熵：表示两个概率分布之间的距离，交叉熵越大，两个概率分布距离越远，两个概率分布越相异计算公式是：$H(y_,y)=-\\sum y_log y$。.0在tensorflow函数表示为：ce= -tf.reduce_mean(y_ tf.log(tf.clip_by_value(y,1e-12,))) softmax函数：将n个分类的n个输出(y1,y2,…,yn)变为满足以下概率分布要求的函数。 学习率 学习率：表示了每次参数更新的幅度大小。学习率过大，会导致优化的参数在最小值附近波动，不收敛；学习率过小，会导致待优化的参数收敛缓慢。在训练过程中，参数的更新向着损失函数梯度下降的方向 参数的更新公式为$w_{n+1}=w_n-learning_rate$ 指数衰减学习率：学习率随着训练轮数变化而动态更新 用Tensorflow表示如下： 1234567global_step= tf.variable(0,trainable = false)learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, #学习率的初始值global_step, #当前训练轮数LEARNING_RATE_STEP, #喂入多少轮后更新学习率staircase为true时 取整数，学习率阶梯型递减；反之 是一条平滑的曲线LEARNING_RATE_DECAY, #学习率衰减率staircase=TRUE/FALSE) 滑动平均记录了一段时间内模型中所有参数w和b各自的平均值。利用滑动平均可以增强模型的泛化能力。 滑动平均值（影子）的计算公式：影子 = 衰减率影子-（1-衰减率）参数。 其中衰减率 = $min\\{MOVING_{AVERAGE_{DECAY’} \\frac{1+轮数}{10+轮数}},影子初值=参数初值\\}$， 用Tensorflow函数表示为 ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step),其中MOVING_AVERAGE_DECAY表示平均滑动衰减率。一般会赋接近1的值，global_step表示当前训练了多少轮。 emp_op = ema.apply (tf.trainable_variables())，其中，emp.apply()函数实现对括号内参数求滑动平均，tf.trainable_variables()函数实现把所有待训练参数汇总为列表 with tf.control_dependencies([train_step,ema_op]): ​ train_op = tf.no_op(name=’train’)其中，该函数实现将滑动平均和训练过程同步进行 正则化 过拟合：神经网络在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较低，泛化能力较差 正则化：在损失函数中给每个参数w加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。使用正则化后，损失函数loss = loss(y,y_)+REGULARIZER*loss(w),其中，第一项是预测结果与标准答案之间的误差，如均方误差、自定义误差函数、交叉熵等；第二项是正则化计算结果。 正则化计算方法： L1正则化：$loss_{L1}=\\sum_i|w_i|$tesnsorflow表示:loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w) L2正则化：$loss_{L2}=\\sum_i|w_i|^2$tesnsorflow表示:loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w) 用tesnsorflow函数实现正则化tf.add_to_collection(‘losses’,tf.contrib.layers.l2_regularizer(regularizer)(w)) loss = cem+tf.add_n(tf.getcollection(‘losses’)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltBATCH_SIZE = 30seed = 2rdm = np.random.RandomState(seed)X = rdm.randn(300,2)Y_ = [int(x0*x0+x1*x1 &lt;2) for(x0,x1) in X]Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]X = np.vstack(X).reshape(-1,2)Y_=np.vstack(Y_).reshape(-1,1)print (X)print (Y_)print (Y_c)plt.scatter(X[:,0],X[:,1],c=np.squeeze(Y_c))plt.show()def get_weight(shape,regularizer): w = tf.Variable(tf.random_normal(shape),dtype=tf.float32) tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w)) return wdef get_bias(shape): b = tf.Variable(tf.constant(0.01,shape=shape)) return bx = tf.placeholder(tf.float32,shape=(None,2))y_ = tf.placeholder(tf.float32,shape=(None,1))w1 = get_weight([2,11],0.01)b1 = get_bias([11])y1 = tf.nn.relu(tf.matmul(x,w1)+b1)w2 = get_weight([11,1],0.01)b2 = get_bias([1])y = tf.matmul(y1,w2)+b2loss_mse = tf.reduce_mean(tf.square(y-y_))loss_total = loss_mse + tf.add_n(tf.get_collection(&apos;losses&apos;))train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) STEPS = 40000 for i in range(STEPS): start = (i*BATCH_SIZE)%300 end = start+BATCH_SIZE sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y_[start:end]&#125;) if i%2000==0: loss_mse_v = sess.run(loss_mse,feed_dict=&#123;x:X,y_:Y_&#125;) print(&quot;After %d steps,loss is:%f&quot; %(i,loss_mse_v)) #xx、yy在-3～3之间以步长为0.01，生成二维网格坐标点 xx,yy=np.mgrid[-3:3:.01,-3:3:.01] grid = np.c_[xx.ravel(),yy.ravel()] probs = sess.run(y,feed_dict=&#123;x:grid&#125;) probs = probs.reshape(xx.shape) print (&quot;w1:\\n&quot;,sess.run(w1)) print (&quot;b1:\\n&quot;,sess.run(b1)) print (&quot;w2:\\n&quot;,sess.run(w2))plt.scatter(X[:,0],X[:,1],c=np.squeeze(Y_c))plt.contour(xx,yy,probs,levels=[.5])plt.show()train_step = tf.train.AdamOptimizer(0.001).minimize(loss_total)with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) STEPS = 40000 for i in range(STEPS): start = (i*BATCH_SIZE)%300 end = start+BATCH_SIZE sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y_[start:end]&#125;) if i%2000==0: loss_v = sess.run(loss_mse,feed_dict=&#123;x:X,y_:Y_&#125;) print(&quot;After %d steps,loss is:%f&quot; %(i,loss_v)) #xx、yy在-3～3之间以步长为0.01，生成二维网格坐标点 xx,yy=np.mgrid[-3:3:.01,-3:3:.01] grid = np.c_[xx.ravel(),yy.ravel()] probs = sess.run(y,feed_dict=&#123;x:grid&#125;) probs = probs.reshape(xx.shape) print (&quot;w1:\\n&quot;,sess.run(w1)) print (&quot;b1:\\n&quot;,sess.run(b1)) print (&quot;w2:\\n&quot;,sess.run(w2))plt.scatter(X[:,0],X[:,1],c=np.squeeze(Y_c))plt.contour(xx,yy,probs,levels=[.5])plt.show()","comments":true,"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/神经网络/"}]},{"title":"搭建神经网络","date":"2019-08-06T18:14:39.693Z","path":"2019/08/07/搭建神经网络/","text":"基本概念 基于tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的参数，得到模型 张量：多维数组，用“阶”来表示张量的维度，“0阶”就表示一个数；“1阶”表示一维数组；“2阶”表示二维数组，以此类推 数据类型： | 数据类型 | Python类型 | 描述 || :—————: | :—————: | :———————————————————————: || DT_FLOAT | tf.float32 | 32位浮点数 || DT_DOUBLE | tf.float64 | || DT_INT64 | tf.int64 | || DT_INT32 | tf.int32 | || DT_INT16 | tf.int16 | || DT_INT8 | tf.int8 | || DT_UINT8 | tf.uint8 | || DT_STRING | tf.string | 可变长度字节数组，每一个张量元素都是一个字节数组 || DT_BOOL | tf.bool | || DT_COMPLEX64 | tf.complex64 | 由两个32位浮点数组成的复数：实数和虚数 || DT_QINT32 | tf.qint32 | 用于量化Ops的32位有符号整数 || DT_QINT8 | tf.qint8 | || DT_QUINT8 | tf.quint8 | | 计算图：搭建神经网络的计算过程，是承载一个或多个计算节点的一张图，但是它只搭建，不运算 12345import tensorflow as tfx = tf.constant([[1.0,2.0]]) #定义一个2阶张量w = tf.constant([[3.0],[4.0]])y = tf.matmul(x,w) #执行矩阵乘法运算print y 上述代码会打印出这样的一句话：Tensor(“matmul:0”,shape(1,1),dtype = float32)。我们可以看出print显示的只是运算过程 会话：执行计算图中的节点运算，我们要使用with结构来实现 12with tf.Session() as sess: print sess.run(y) 这时我们可以看到打印出来的是11。 神经网络的参数 神经网络上的参数：指神经元线上的权重w，用变量表示，一般会先随机生成这些参数。生成参数的方法是让w=tf.Variable.常用的生成随机函数的方式有： | 函数名 | 描述 || :—————————-: | ———————————————— || tf.random_normal() | 生成正态分布函数 || tf.truncated_normal() | 生成去掉过大偏离点的正态分布函数 || tf.random_uniform() | 生成均匀分布随机数 || tf.zeros | 生成全0数组 || tf.ones | 生成全1数组 || tf.fill | 生成全定值数组 || tf.constant | 生成直接给定值的数组 | 例： tf.variable(tf.random_normal([2,3],stddev=2,mean=0,seed=1))[2,3]表示生成的随机数形状是2行3列的，标准差是2，均值是0，随机种子是1 神经网络的搭建 准备数据集，提取特征，作为输入喂给神经网络(NN) 搭建神经网络结构(NN)，从输入到输出，即：先搭建计算图，然后再会话执行 大量特征数据喂给NN,迭代优化NN参数 使用训练好的模型预测和分类 ​ 对于变量的初始化，我们需要在sess.run中写入tf.global_variables_initializer实现对所有变量的初始化 前向神经网络 变量初始化：sess.run(tf.global_variables_initializer()) 图节点运算：sess.run(y) 在构建计算图的时候，我们有时候不知道要传多少值进去，这时就可以使用tf.placeholder占位 1234x=tf.placeholder(tf.float32,shape=(1,2)) # 表示传的是1*2的矩阵sess.run(y,feed_dict=&#123;x:[[0.5,0.6]]&#125;) #feed_dict 喂入数据tf.placeholder(tf.float32,shape=(none,2))# 表示n*2的矩阵sess.run(y,feed_dict=&#123;x:[[0.1,0.3],[0.8,0.4],、、、,[0.4,0.1]]&#125;) 反向传播 意义：训练参数模型，在所有参数上用梯度下降，使得NN模型在训练数据上的损失函数最小 损失函数LOSS：计算得到的预测值y_和真实值的差距 均方误差MSE：是损失函数的一种计算方法，求前向传播计算结果与已知答案之差的平方再求平均$MSE(y_,y) = \\frac{\\sum_{i=1}^n(y-y_)^2}{n}$，用tensorflow的表示为tf.reduce_mean(tf.square(y_-y)) 反向传播训练方法：以减小loss值为优化目标，有梯度下降，momentum优化器，adam优化器等分别表示为 123train_step = tf.train.GradientDescentOptimizer(learning_rate),minimize(loss)train_step = tf.train.MomentumOptimizer(learning_rate,momentum).minimize(loss)train_strp = tf.train.AdamOptimizer(learning_rate).minimize(loss) 使用梯度下降法，使参数沿着梯度反向，即总损失减小的方向，实现参数更新更新公式：$\\theta_{n+1} = \\theta_n-\\alpha\\frac{\\alpha J(\\theta_{i-1})}{\\alpha\\theta_n}$.其中$J(\\theta)$为损失函数，$\\theta$为参数 $\\alpha $为学习率 在更新参数时，利用了超参数，更新公式是：$d_i = \\beta d_{i-1}+g(\\theta_{i-1})$，$\\theta_i = \\theta_{i-1}-ad_i$，其中$g(\\theta_{i-1})$为损失函数，$\\theta$为参数, $\\alpha $为学习率,$\\beta$是超参数 超参数是什么？超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果 利用自适应学习率的优化算法，Adam和随机梯度下降法不同。随机梯度下降法保持单一的学习率更新所有的参数，学习率在训练过程中并不会改变。而adam算法通过计算梯度的一阶矩和二阶矩估而为不同的参数设计独立的自适应学习率。 学习率：决定每次参数更新的幅度。从上面的代码中，我们可以看到优化器中都需要一个学习率的参数，如果学习率过大会出现震荡不收敛的情况，反之，会出现收敛速度太慢，一般可以填一个比较小的值。","comments":true,"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/神经网络/"}]},{"title":"克里金估计技术的基本原理","date":"2019-08-04T03:56:33.462Z","path":"2019/08/04/克里金估计技术的基本原理/","text":"克里金估计技术的基本原理1.研究对象和目的​ 克里金技术的研究对象是空间预测问题，就是利用空间中的一个或者几个变量的观测数据，来估计出这个或者这些变量在该空间中的随位置不同而变化的规律和特征。 ​ 在进行空间数据的预测时会遇到如下几个方面的困难： 由于观测位置和观测数据方法的局限性，以及观测过程中的人为因素，观测所得到的数据都有不同程度的误差 观测数据在空间分布往往是不规则，很难有一定的规律性 和一个变量有关的观测数据往往不止一种，有的直接，有的间接；有的数目较多，有的较少；有的空间变化比较平缓，有的比较剧烈。 空间预测的条件、环境、目的和要求往往是不一样的。 因此需要发展多种方法来适应这种情况 2.数学模型——随机过程及其估计​ 随机过程可以看成是一个随机函数。随机过程的本质有两点：1.随机，2.过程。随机说明任何时候结果都存在随机性，是不确定的；过程体现的是一个时间上的维度，在t1时刻是随机变量服从某一个分布，而到了t2时刻又是服从其他分布。 ​ 随机变量X代表着一种对应关系，对于一个随机试验的每个结果₰，都可以赋予一个数值X(₰)，然而随机过程X(t)则是对于每个₰赋予一个函数X(t，₰)的一种对应关系，所以随机过程是依赖于参量₰的一族时间函数，即他是t和₰的函数。 ​ 如果用X(t)表示随机过程，以省略其对₰的依赖关系，则X(t)具有如下涵义： 它是一族自变量为t和₰的函数X(t，₰) 当₰固定时，X(t，₰)表示的是该过程的一个实现，仅仅是时间t的函数 当t固定时，而₰是变量时，那么X(t)是一个随机变量，代表着该过程在时刻t下的状态 当t和₰都固定时，那么X(t)就是一个确定的数值 随机过程的基本特征：统计特性： 事实上，随机过程是由可数的无穷多个随机变量组成，其中的每一个随机变量对应一个t。对于特定的t，X(t)是一个随机变量，其分布函数为$ F(x,t) =P\\begin{cases}X(t)\\leq x\\end{cases}\\}$ ,此分布函数依赖于t，它等于事件$|x(t)\\leq x|$的概率。该事件是在是在特定的时间t，在该过程的样本$x(t,\\zeta)$中不超过数值x的全部结果$X(t,\\zeta _i)$所组成。 随机过程的数学期望，自相关函数和协方差函数均表达了随机过程的统计特性。 数学期望：它反应随机变量平均取值的大小，对于连续形随机变量$\\zeta$的可能取值为$(-\\infty,+\\infty)$,所以随机过程$E(t)$的数学期望的表达式为$\\eta(t)=E[X(t)]=\\int_{-\\infty}^{+\\infty}xf(x,t)d_x$;对于离散形随机变量$E[X]=\\sum_{k=-\\infty}^{+\\infty}x_kp_k$, 自相关函数：均值和方差只能刻画出随机过程在各个独立时刻的概率统计特性，反应不了随机过程的内在相关性，所以引入了自相关函数，它是为了分辨形状不同的$x(t)$，所以带入$E\\{X(t_1)E(t_2)\\}$公式的值应该就是取两个时刻的函数值自己相乘，这样才能反映出$x(t)$的差异。$R(t_1,t_2)=E[X(t_1)\\cdot E(t_2)]=\\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}x_1x_2f(x_1,x_2;t_1.t_2)d_{x_1}d_{x_2}$. 自协方差函数：和自相关函数差不多，描述随机变量$X(t)$在两个不同时刻的取值的起伏变化的相关程度。 $C(t_1,t_2)=E\\{ \\{X(t_1)-E[X(t_1)]\\} \\{X(t_2)-E[X(t_2)]\\} \\}=R(t_1,t_2)-\\eta(t_1)\\eta(t_2)$ 平稳性：是否具有平稳性是随机过程的一个重要性质，具有平稳性的随机过程叫做平稳随机过程。 严格平稳：$F(x_1,x_2,\\cdots,x_n;t_1,t_2,\\cdots,t_n)=F(x_1,x_2,\\cdots,x_n;t_1+c,t_2+c,\\cdots,t_n+c)$即将邻域当成重复取样点，这个假设通常情况下是不符合实际的 二阶平稳：当区域化变量$Z(u)$满足下列2个条件时，称其为二阶平稳或弱平稳 在整个研究区内$Z(u)$的数学期望存在，且是常数，即$E[Z(u)=E[Z(u+h)]=m] \\quad \\forall A \\quad\\forall h$,随机函数在空间上没有明显的变化趋势。 在整个研究区内，$Z(u)$的协方差存在且平稳。即只依赖于滞后h，而与u无关，$Cov\\{Z(u),Z(u+h\\}=E[Z(u)Z(u+h)]-E[Z(u)]E[Z(u+h)]=E[Z(u)Z(u+h)]-m^2=C(h)$，协方差不依赖空间绝对位置，而依赖于相对位置，即空间具有稳定不变性。 本征假设(内蕴假设):它是比二阶平稳假设更弱的假设。 各态历经性：为了求取$X(t)$的数学期望函数，按照概率论的方法，我们需要取得该过程的大量实现，然后求算术平均值，才能得到数学期望函数。但是在实际情况下，很难得到一个过程各个随机变量的大量实现，也就没法通过算术平均来求得数学期望函数。因此需要采用尽可能少的实现，去求取数学期望等各种统计量。对此，具有各态历经性的平稳过程允许用一个实现的时序平均去代替相应过程的总体平均。 估值理论： 最小方差估计 最大验后估计 贝叶斯估计 最大似然估计 线性最小方差估计 空间变量的随机模型及其预测克里金技术解决空间预测问题的过程 将所研究空间变量与一个随机函数$Z(x)$相对应，且让它们的定义域相同，进一步再将空间变量在$x_1,x_2,\\cdots,x_n$等处的观测值$Z(x_1),Z(x_2),\\cdots,Z(x_n)$分别视为随机变量$Z(x_1),Z(x_2),\\cdots,Z(x_n)$的一个实现 利用估计理论，根据$Z(x_1),Z(x_2),\\cdots,Z(x_n)$对随机函数$Z(x)$在$$处的随机变量进行估计，其结果可形式地计为$Z^{*}(x_0)=f[Z(x_1),Z(x_2),\\cdots,Z(x_n)]$，关键是求出函数关系f 将观测值$Z(x_1),Z(x_2),\\cdots,Z(x_n)$分别代替f中的$Z(x_1),Z(x_2),\\cdots,Z(x_n)$，从而得到克里金估计值$Z^*(x_0)$. 线性预测由于克里金技术所研究的空间变量随空间位置不同而变化的复杂性，以及观测数据数目较少而造成的信息不充分，因此需要将函数的一般形式明确完全地确定下来是不可能的，所以我们需要对估计量的形式进行简化，将函数关系f取成$Z(x_1),Z(x_2),\\cdots,Z(x_n)$的线性函数。这样就有了 Z^*(x_0)=\\lambda_0+\\sum_{i=1}^n\\lambda_iZ(x_i)\\quad\\quad(1)为了确定$\\lambda i$的值，需要确定用什么方法去估计，一般采用的是最小方差估计，即： E\\{[Z(x_0)-Z^*(x_0)]^2\\}=min \\quad\\quad(2)当$Z^(x_0)$能够被$Z(x_1),Z(x_2),\\cdots,Z(x_n)$的线性最小方差估计时，获取$Z^(x_0)$的系数也就可以根据$Z(x_1),Z(x_2),\\cdots,Z(x_n)$的一阶矩和二阶矩表示出来，而无需涉及它们的概率密度函数。 先来讨论$Z(x)$的一阶矩如下的三种情况进行讨论 $Z(x)$的数学期望$m(x)=E[Z(x)]$,对所有的x，都是已知的，可以允许当$x!=x_0时,m(x)!=m(x_0)$。 $Z(x)$的数学期望$m(x)$是一个和x无关的常数，但属未知，这时随机函数Z(x)为一阶平稳 $Z(x)$的数学期望$m(x)$即不平稳也不可知 简单克里金(一阶矩是第一种的情况)如果$Z^(x_0)$是线性最小方差估计，那么$Z^(x_0)$是$Z(x_0)$的无偏估计。 E[Z(x_0)-Z^*(x_0)]=E\\{Z(x_0)-[\\lambda_0+\\sum_{i=1}^n\\lambda_iZ(x_i)]\\} \\\\ =m_0-\\lambda_0-\\sum_{i=1}^n\\lambda_im_i=0\\quad\\quad(3)其中$m_i=E[Z(x_i)],i=0,1,2,\\cdots,n$。那么当$\\lambda_i$全部确定后，可以确定$\\lambda_0$为： \\lambda_0 = m_0-\\sum_{i=1}^n\\lambda_im_i\\quad\\quad(4)由（3）交换律。从而有$Z^*(x_0)=m_0+\\sum_{i=1}^n\\lambda_i[Z(x_i)-m_i]\\quad\\quad(5)$ 令$a_0=1,a_i=-\\lambda_i$利用（5）可以得到 Z(x_0)-Z^*(x_0) = \\sum_{i=0}^na_i[Z(x_i)-m_i]\\quad\\quad(6)由（5），估计误差的数学期望为0，从而估计误差的方差为 E\\{[Z(x_0)-Z^*(x_0]^2\\} = E \\{ \\{\\sum_{i=0}^na_i[Z(x_i)-m_i]\\}^2=\\sum_{i=0}^n\\sum_{j=0}^na_ia_jC(x_i,x_j)\\}\\quad\\quad(7)其中$C(x_i,x_j)$就是协方差函数。如果在（7）式中有$\\lambda_1,\\lambda_2,\\cdots,\\lambda_n$的一组数值，使得估计方差为最小，那么这组数值就使下列式成立 \\frac{1}{2}\\frac{\\alpha}{\\alpha\\lambda_j}E\\{[Z(x_0)-Z^*(x_0)]^2\\} \\\\=\\sum_{i=0}^na_iC(x_i,x_i)=0 \\\\由于a_0=1,a_i=-\\lambda_i，那么就有\\\\\\sum_{i=1}^n\\lambda_iC(x_i,x_j) = C(x_0,x_j)\\quad\\quad(8)公式8就是简单克里金方程组，将求得得到$\\lambda_i$带入公式5，即可得到简单克里金的估计量$Z^*(x_0)$。事实上，公式5和6中的$m_i$的假设是已知的。 当然也可以写成矩阵的形式： K\\lambda=k \\quad\\quad (9) K=\\begin{bmatrix} C(x_1,x_1) & C(x_1,x_2) & \\cdots & C(x_1,x_n) \\\\ C(x_2,x_1) & C(x_2,x_2) & \\cdots & C(x_2,x_n) \\\\ \\cdots &\\cdots &\\cdots & \\cdots \\\\ C(x_n,x_1) & C(x_n,x_2) & \\cdots &C(x_n,x_n) \\end{bmatrix} K^T = (C(x_0,x_1),\\cdots,C(x_0,x_n ))普通克里金（一阶矩是第二种）假设$Z(x)$是一阶平稳的，数学期望是常数m，但是当前还不知道具体是多少。 这里面的$Z^(x_0)$仍然采用公式1的形式。只是$\\lambda_0=0$,那么$Z^(x_0)$无偏线性估计量可以写成 \\begin{equation} \\begin{cases} Z^*(x_0)=\\sum_{i=1}^n\\lambda_iZ(x_i) \\\\ \\sum_{i=1}^n\\lambda_i = 1 \\end{cases} \\end{equation} \\quad\\quad (11)此时再令$a_0=1,a_i=-\\lambda_i$ 可以得到 Z(x_0)-Z^*(x_0) = \\sum_{i=0}^na_i[Z(x_i)-m_i] E\\{[Z(x_0)-Z^*(x_0)]^2\\} =\\sum_{i=0}^n\\sum_{j=0}^na_ia_jC(x_i,x_j)\\\\ =C(x_0,x_0)-2\\sum_{j=1}^n\\lambda_jC(x_0,x_j)+\\sum_{i=1}^n\\sum_{j=1}^n\\lambda_i\\lambda _jC(x_i,x_j)为了在$\\sum_{i=1}^n\\lambda_i=1$的约束条件下，满足估计方差最小，需要用Lagrange乘数法去构造如下函数$F=E\\{[Z(x_0)-Z^*(x_0)]^2\\}-2\\mu(\\sum_{i=1}^n\\lambda_i-1)$。则有 \\begin{equation} \\begin{cases} \\frac{1}{2}\\frac{\\alpha F}{\\alpha\\lambda_i}=\\sum_{i=1}^n\\lambda_iC(x_i,x_j)-\\mu-C(x_0,x_j) \\\\ \\frac{1}{2}\\frac{\\alpha F}{\\alpha\\mu} =-(\\sum_{i=1}^n\\lambda_i-1)=0 \\end{cases} \\end{equation} \\quad\\quad (11)改写一下： \\begin{equation} \\begin{cases} \\sum_{i=1}^n\\lambda_iC(x_i,x_j)-\\mu=C(x_0,x_j) \\\\ \\sum_{i=1}^n\\lambda_i = 1 \\end{cases} \\end{equation} \\quad\\quad (12)这就是普通克里金方程组。 克里金方程组的性质及其求解从推导出的简单克里金方程组和普通克里金方程组可以看出，为了求这些方程组的解$\\lambda_i$，必须具备两个条件： 确定方程组的系数矩阵和右端项 方程组的系数矩阵是非奇异的，从而方程组具有唯一解。所谓非奇异矩阵就是行列式不为0的矩阵，也就是可逆矩阵 在前面介绍的二阶平稳性假设和内蕴假设就是为了解决克里金方程组的系数矩阵和右端项的问题。 为了确定克里金方程组的系数矩阵以及右端项，就必须要求取随机变量$Z(x_i)$和$Z(x_j)$的协方差函数 $C(x_i,x_j)$，然而，由于以下两个原因，要利用观测数据直接求协方差函数是几乎不可能的： $E[Z(x_i)]$是未知的，在普通克里金估计中，我们假设它就是未知的 在一般情况下，不可能在空间变量进行重复的多次测量，即在同一个空间变量上只会测量一次，而二阶平稳的条件能够简化随机函数$Z(x)$的协方差函数的结构。该协方差函数仅仅依赖于两点之间的差向量。则有 C(h)=E\\{ \\{ Z(x+h)-E[Z(x+h)]\\} \\{Z(x)-E[Z(x)]\\} \\}=E[Z(x+h)Z(x)]-m^2\\quad\\quad(13)所以普通克里金方程组也可以改写成如下形式： \\begin{cases} \\sum_{i=1}^n\\lambda_iC(x_i-x_j)-\\mu = C(x_j-x_0),j=1,2,\\cdots,n \\\\ \\sum_{i=1}^n\\lambda_i = 1 \\end{cases}\\quad\\quad(14)变异函数由于计算$C(x_i-x_j)$和$C(x_j-x_0)$，必须要知道$E[Z(x_i)]$的数值m，所以还是无法直接求取系数。 变异函数是克里金估计一个重要的且有特殊地位的概念。它作为随机变量$[Z(x_1)-Z(x_2)]^2$的数学期望，可定义如下： 2\\gamma(x_1,x_2) = E\\{[Z(x_1)-Z(x_2)]^2\\} \\quad\\quad(15)当随机函数是二阶平稳，则有： \\gamma(x_1,x_2) = \\frac{1}{2}E\\{[Z(x_1)-Z(x_2)]^2\\} \\\\ =C(0)-C(x_1-x_2)\\quad\\quad(16)将公式17带入到14 \\begin{cases} \\sum_{i=1}^n\\lambda_i\\gamma(x_i-x_j)+\\mu = \\gamma(x_j-x_0) \\\\ \\sum_{i=1}^n\\lambda_i = 1 \\end{cases} \\quad\\quad(17)得到利用变异函数作为克里金方程组系数的优点在于不需要再去求取$E[Z()x_i]$了。 内蕴假设 二阶平稳的随机函数具有协方差函数，所以会存在一个有限的先验方差Var{|Z(x)|}=C(0)。然而像布朗运动等许多物理现象的随机函数具有无限大的离散性，也就是说既无先验方差，也没有协方差函数。因此需要把随机函数的二阶平稳条件给放松。 一个定义在$\\Omega$上的随机函数需要满足2个条件 $E\\{Z(x+h)-Z(x)\\}$=0 对于所有定义在$\\Omega$上的向量h，随机函数的增量[Z(x+h)-Z(x)]有一个与x无关的有限方差，记为$2\\gamma(h)=Var\\{Z(x+h)-Z(x)\\}=E\\{[Z(x+h)-Z(x)]^2\\}$. 在推导内蕴条件下的方程组时，由于协方差可能不存在，只能用变异函数 对于满足内蕴条件的随机函数，我们可以再定义另外的一个随机函数$Z’(x)=Z(x)-Z(x_0)$其中x0是空间中固定的一点，则变异函数可以写成如下形式 2\\gamma(x_i-x_j) = E\\{[Z(x_i)-Z(x_j)]^2\\}\\\\ =E\\{ \\{[Z(x_i)-Z(x_0)]-[Z(x_j)-Z(x_0)]\\}^2 \\} \\\\ =E\\{[Z(x_i)-Z(x_0)]^2\\}+E\\{[Z(x_j)-Z(x_0)]^2\\}-2E\\{[Z(x_i)-Z(x_0)][Z(x_j)-Z(x_0)]\\} \\\\ =2\\gamma(x_i-x_0)+2\\gamma(x_j-x_0)-2C'(x_i-x_j) \\quad\\quad(19)则方差也要改写为： E\\{[\\sum_{i=1}^n\\lambda_iZ(x_i)-Z(x_0)]^2\\} \\\\ =E\\{ \\{\\sum_{i=1}^n\\lambda_i[Z(x_i)-Z(x_0)]\\}^2\\}\\\\ =\\sum_{i=1}^n\\sum_{j=1}^n\\lambda_i\\lambda_jE\\{[Z(x_i)-Z(x_0)]-[Z(x_i)-Z(x_0)]\\} \\quad\\quad(20)此式的两端对$\\lambda_i$求偏导，再结合19 得到 \\frac{\\alpha}{\\alpha\\lambda_j}E\\{[\\sum_{i=1}^n\\lambda_iZ(x_i)-Z(x_0)]^2\\}\\\\ =\\frac{\\alpha}{\\alpha\\lambda_j}\\sum_{i=1}^n\\sum_{j=1}^n\\lambda_i\\lambda _j[\\gamma(x_i-x_0)+\\gamma(x_j-x_0)-\\gamma(x_i-x_j)] \\\\ =-\\sum_{i=1}^n\\lambda_i\\gamma(x_i-x_j)+\\gamma(x_i-x_0) \\quad\\quad(21)因此，可以再令 \\frac{\\alpha}{\\alpha\\lambda_j}\\{E\\{[\\sum_{i=1}^n\\lambda_iZ(x_i)-Z(x_0)]^2\\}-\\mu(\\sum_{i=1}^n\\lambda_i-1)\\}=0 \\\\ \\frac{\\alpha}{\\alpha\\mu}\\{E\\{[\\sum_{i=1}^n\\lambda_iZ(x_i)-Z(x_0)]^2\\}-\\mu(\\sum_{i=1}^n\\lambda_i-1)\\}=0 得到了相应的克里金方程组 \\begin{cases} \\sum_{i=1}^n\\lambda_i\\gamma(x_i-x_j)+\\mu = \\gamma(x_j-x_0) \\\\ \\sum_{i=1}^n\\lambda_i = 1 \\end{cases} \\quad\\quad(22)可以看到在内蕴假设下推导出的克里金方程组和在二阶平稳条件下推导出的克里金方程组是一致的。 未完待续","comments":true,"tags":[{"name":"-地质统计 -克里金","slug":"地质统计-克里金","permalink":"http://yoursite.com/tags/地质统计-克里金/"}]},{"title":"我的第一篇博客","date":"2019-06-30T13:36:08.000Z","path":"2019/06/30/我的第一篇博客/","text":"第一部分哈哈哈第二部分呵呵呵第三部分嘻嘻嘻 参考文献**codesheep","comments":true,"tags":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]}]